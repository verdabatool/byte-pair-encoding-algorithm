{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "# Task 2: The Byte Pair Encoding Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "part-1"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting up the foundations\n",
    "\n",
    "### The BPE Algorithm\n",
    "\n",
    "The Byte Pair Encoding algorithm is an iterative approach to building a subword vocabulary. It starts by treating each character in the training corpus as an initial token. Then, it repeatedly finds the most frequent pair of adjacent tokens and merges them into a new single token. This process is repeated for a specified number of merges, which determines the final vocabulary size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scenario-1"
   },
   "source": [
    "### Scenario 1: A Minimalist Implementation\n",
    "\n",
    "Your task is to implement the core BPE algorithm for a small, controlled text corpus.\n",
    "\n",
    "#### Problem Description\n",
    "\n",
    "You are given the following training text:\n",
    "\n",
    "`'a b c a b c a a b c a a'`\n",
    "\n",
    "Your task is to implement a function that performs a single BPE merge on this text.  \n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "1. Initialize the vocabulary. Each character is a token.\n",
    "2. Count the frequency of all adjacent token pairs.\n",
    "3. Identify the most frequent pair.\n",
    "4. Merge the most frequent pair into a new token.\n",
    "5. Return the new token list and the new vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "code-1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Tokens: ['a', 'b', 'c', 'a', 'b', 'c', 'a', 'a', 'b', 'c', 'a', 'a']\n",
      "Initial Vocab: {'c', 'a', 'b'}\n",
      "Pair counts: Counter({('a', 'b'): 3, ('b', 'c'): 3, ('c', 'a'): 3, ('a', 'a'): 2})\n",
      "Most Frequent Pair: ('a', 'b')\n",
      " ------ After FIRST merge ------ \n",
      "New tokens: ['ab', 'c', 'ab', 'c', 'a', 'ab', 'c', 'a', 'a']\n",
      "New Vocab: {'c', 'ab', 'a', 'b'}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 1.Spliting into tokens and initializing vocabulary\n",
    "given_text = 'a b c a b c a a b c a a'\n",
    "tokens = given_text.split() # this will be generating character level tokens\n",
    "print(f\"Initial Tokens: {tokens}\")\n",
    "vocab = set(tokens)\n",
    "print(f\"Initial Vocab: {vocab}\")\n",
    "pairs = []\n",
    "\n",
    "# 2. Counting frequency of all adjacent token pairs\n",
    "for i in range(len(tokens)-1):\n",
    "    pair = (tokens[i], tokens[i+1])\n",
    "    pairs.append(pair)\n",
    "pair_counts = Counter(pairs)\n",
    "print(f\"Pair counts: {pair_counts}\")\n",
    "\n",
    "# 3.identifying the most frequent pair\n",
    "most_frequent_pair = pair_counts.most_common(1)[0][0] \n",
    "\n",
    "print(f\"Most Frequent Pair: {most_frequent_pair}\")\n",
    "print(\" ------ After FIRST merge ------ \")\n",
    "\n",
    "# 4. Merging the most frequent pair into a new token\n",
    "merging_most_frequent_pair = \"\".join(most_frequent_pair)\n",
    "\n",
    "# 5. New tokens\n",
    "i = 0\n",
    "new_tokens = []\n",
    "while i < len(tokens):\n",
    "    if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == most_frequent_pair:\n",
    "        new_tokens.append(merging_most_frequent_pair)\n",
    "        i += 2\n",
    "    else:\n",
    "        new_tokens.append(tokens[i])\n",
    "        i +=1\n",
    "print(f\"New tokens: {new_tokens}\")\n",
    "\n",
    "# 6. Returning a new vocabulary\n",
    "vocab.add(merging_most_frequent_pair)\n",
    "new_vocab = vocab\n",
    "print(f\"New Vocab: {new_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "questions-1"
   },
   "source": [
    "## Reflective Questions\n",
    "\n",
    "**1. Why is it important to start with characters as the initial tokens?**  \n",
    "Starting with characters as tokens provides a meaningful base vocabulary. It ensures that algorithm has seen those words which are not even there during training. Through adjacent merges individual character as tokens can be converted into syllables, words, sub words.  \n",
    "\n",
    "**2. What is the purpose of counting adjacent pairs?**  \n",
    "It helps in identifying the most frequent sequences occuring together. By merging the most common pairs, the algorithm gradually builds larger, more meaningful tokens that reduce sequence length and improve efficiency.  \n",
    "\n",
    "**3. If there were a tie for the most frequent pair, how would you resolve it? What is a common strategy in BPE implementations?**. \n",
    "There are multiple ways of resolution in case of a tie for the most frequent pair e-g. picking the first occuring pair, or use lexicographic ordering. In case of my implementation i have used lexicographic ordering for picking up the most frequent pair in case of a tie.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "part-2"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 2: Extending the Implementation\n",
    "\n",
    "### Scenario 2: Training a BPE Model\n",
    "\n",
    "Now, you will build upon your initial implementation to create a full BPE training function. This function will take a text corpus and a specified number of merges as input and will generate a vocabulary and a tokenizer.  \n",
    "\n",
    "#### Implementation Steps\n",
    "1. Implement the `train` function to obtain the tokens for the given text corpus. The function should return the final vocabulary of the tokenizer after it has been trained.\n",
    "2. Iteratively perform `num_merges` merges. \n",
    "In each iteration:  \n",
    "    a. Count pair frequencies.  \n",
    "    b. Identify and store the most frequent pair and its new token.  \n",
    "    c. Merge the pair throughout the text corpus.  \n",
    "3. Implement the `tokenize` function that takes a new string as input and tokenizes it using the learned merges and vocabulary. This function must handle new words by breaking them down into known subwords or characters.  \n",
    "\n",
    "#### Problem Statement\n",
    "You have to perform the following tasks:\n",
    "\n",
    "- Train the tokenizer on the following text:\n",
    "\n",
    "    `'the dog is a good boy, the cat is a good girl'`\n",
    "\n",
    "    - Print the vocabulary of the tokenizer\n",
    "    - Use num_merges = 5\n",
    "\n",
    "- Tokenize the following sentence and output the result:\n",
    "\n",
    "    `'the good dog is a boy'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "code-2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Tokens: ['t', 'h', 'e', ' ', 'd', 'o', 'g', ' ', 'i', 's', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'b', 'o', 'y', ',', ' ', 't', 'h', 'e', ' ', 'c', 'a', 't', ' ', 'i', 's', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'g', 'i', 'r', 'l']\n",
      "Initial Vocab: {'o', ',', 's', 'g', 'r', 'h', 'l', 'y', ' ', 'a', 'c', 'e', 't', 'b', 'i', 'd'}\n",
      "\n",
      "Vocabulary after training: {',', 's', 'g', 'r', 'the ', 'l', 'y', 'e', 'd', 'h', ' i', ' ', 'a', 'o', 'c', 'b', ' g', 'th', 'the', 't', 'i'}\n",
      "Tokenized sentence: ['the', ' g', 'o', 'o', 'd', ' ', 'd', 'o', 'g', ' i', 's', ' ', 'a', ' ', 'b', 'o', 'y']\n"
     ]
    }
   ],
   "source": [
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "        self.merges = []\n",
    "\n",
    "    def train(self, text_corpus, num_merges):\n",
    "        # 1. Initializing tokens and vocabulary (character level)\n",
    "        tokens = list(text_corpus)  # keeping spaces\n",
    "        self.vocab = set(tokens)\n",
    "        print(f\"Initial Tokens: {tokens}\")\n",
    "        print(f\"Initial Vocab: {self.vocab}\\n\")\n",
    "        for merge_num in range(num_merges):\n",
    "            pairs = []\n",
    "\n",
    "            # 2. Counting frequency of adjacent pairs\n",
    "            for i in range(len(tokens)-1):\n",
    "                pair = (tokens[i], tokens[i+1])\n",
    "                pairs.append(pair)\n",
    "            pair_counts = Counter(pairs)\n",
    "            if not pair_counts:\n",
    "                break\n",
    "\n",
    "            # 3. Finding the most frequent pair\n",
    "            most_frequent_pair = pair_counts.most_common(1)[0][0]\n",
    "            self.merges.append(most_frequent_pair)\n",
    "            merged_token = \"\".join(most_frequent_pair)\n",
    "\n",
    "            # 4. Merging the pair in token list\n",
    "            i = 0\n",
    "            new_tokens = []\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens)-1 and (tokens[i], tokens[i+1]) == most_frequent_pair:\n",
    "                    new_tokens.append(merged_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = new_tokens\n",
    "\n",
    "            # 5. Updating vocabulary\n",
    "            self.vocab.add(merged_token)\n",
    "            # print(f\"--- Merge {merge_num} ---\")\n",
    "            # print(f\"Pairs: {pairs}\")\n",
    "            # print(f\"Pair Counts: {pair_counts}\")\n",
    "            # print(f\"Most Frequent Pair: {most_frequent_pair}\")\n",
    "            # print(f\"Updated Tokens: {tokens}\")\n",
    "            # print(f\"Updated Vocab: {self.vocab}\\n\")\n",
    "        return self.vocab\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \n",
    "        # Starting from character-level tokens including spaces\n",
    "        tokens = list(text)\n",
    "        for pair in self.merges:\n",
    "            merged_token = \"\".join(pair)\n",
    "            i = 0\n",
    "            new_tokens = []\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens)-1 and (tokens[i], tokens[i+1]) == pair:\n",
    "                    new_tokens.append(merged_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "text_corpus = 'the dog is a good boy, the cat is a good girl'\n",
    "bpe = BPE()\n",
    "\n",
    "# Training with 5 merges\n",
    "vocab = bpe.train(text_corpus, num_merges=5)\n",
    "print(\"Vocabulary after training:\", vocab)\n",
    "\n",
    "# Tokenizing a new sentence\n",
    "sentence = 'the good dog is a boy'\n",
    "tokens = bpe.tokenize(sentence)\n",
    "print(\"Tokenized sentence:\", tokens)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "questions-2"
   },
   "source": [
    "## Reflective Questions\n",
    "\n",
    "**1. How does the number of merges affect the size of the final vocabulary?**  \n",
    "As the number of merges increase, the size of final vocabulary increases. For instance, at the start the initial vocabulary is just the characters. As there are merges happening, the updated vocabulary is individual characters + merged most frequent pairs as tokens.  \n",
    "\n",
    "**2. How does your tokenizer handle out-of-vocabulary (OOV) words? For example, if the word 'supercalifragilisticexpialidocious' were encountered, how would it be tokenized by your model?**  \n",
    "In case of BPE the algorithm starts with the individual character level tokens and then perform frequent merges. Hence, no word is fully known. In case, there is some new word in testing phase the algorithm will still apply all the tokens learned during training. e-g. if the model lerned merges like \"su\", \"per\", \"cal\" during training phase and the new word 'supercalifragilisticexpialidocious' appears in the test phase (case of OOV), then the tokenization would look like this (learned tokens + the individual characters of the new word)    \n",
    "['su', 'per', 'cal', 'i', 'f', 'r', 'a', 'g', 'i', 'l', 'i', 's', 't', 'i', 'c','e','x', 'p','i', 'a', 'l', 'i', 'd', 'o', 'c', 'i', 'o', 'u', 's']\n",
    "\n",
    "\n",
    "**3. What are the trade-offs between having a very large vocabulary (many merges) and a very small one (few merges)? Consider the impact on model size and the representation of rare words.**  \n",
    "- In case of a large vocabulary, we have fewer tokens per sentence and common words/sub-words are stored directly. Due to these reasons, we have faster training. This means that such a model will not generalize well to rare words because of less fallback to character level.  \n",
    "- Small vocabulary (less merges) mean that we have a compact model (smaller embeddings). OOV or rare words will be better handled as they will be broken down into individula level charcters. But this also means that during tokenization, a sentence will be broken down into many tokens (longer sequences) and hence, training will be slower.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "part-3"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Impact of Corpus Size and Type\n",
    "\n",
    "### Scenario 3: Training on a Different Corpus\n",
    "\n",
    "This scenario explores how the characteristics of the training data influence the resulting BPE model.  \n",
    "\n",
    "#### Problem Description\n",
    "\n",
    "You are given two different text corpora.\n",
    "\n",
    "- Corpus A: `'a b c a b c a a b c a a'`\n",
    "- Corpus B: `'the dog ate the food, the cat ate the mouse'`\n",
    "\n",
    "Train two separate BPE models, one for each corpus, with `num_merges = 3`. Analyze the resulting vocabularies and the merges learned.  \n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "1. Use your `train` function from Part 2.\n",
    "2. Train a BPE model on Corpus A with 3 merges. Print the final vocabulary.\n",
    "3. Train a BPE model on Corpus B with 3 merges. Print the final vocabulary.\n",
    "4. Analyze the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "steps-3-implementation"
   },
   "source": [
    "#### Implementation Steps\n",
    "\n",
    "1. Use your `train` function from Part 2.\n",
    "2. Train a BPE model on Corpus A with 3 merges. Print the final vocabulary.\n",
    "3. Train a BPE model on Corpus B with 3 merges. Print the final vocabulary.\n",
    "4. Analyze the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "code-3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE on Corpus A...\n",
      "\n",
      "Initial Tokens: ['a', ' ', 'b', ' ', 'c', ' ', 'a', ' ', 'b', ' ', 'c', ' ', 'a', ' ', 'a', ' ', 'b', ' ', 'c', ' ', 'a', ' ', 'a']\n",
      "Initial Vocab: {' ', 'b', 'a', 'c'}\n",
      "\n",
      "Final Vocabulary for Corpus A: {'a b ', ' ', 'a', 'c', 'a ', 'b', 'a b'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Training BPE on Corpus B...\n",
      "\n",
      "Initial Tokens: ['t', 'h', 'e', ' ', 'd', 'o', 'g', ' ', 'a', 't', 'e', ' ', 't', 'h', 'e', ' ', 'f', 'o', 'o', 'd', ',', ' ', 't', 'h', 'e', ' ', 'c', 'a', 't', ' ', 'a', 't', 'e', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'u', 's', 'e']\n",
      "Initial Vocab: {'o', ',', 's', 'g', 'h', 'f', ' ', 'm', 'a', 'c', 'e', 't', 'u', 'd'}\n",
      "\n",
      "Final Vocabulary for Corpus B: {'o', ',', 's', 'g', 'th', 'h', 'f', 'e ', 'the ', ' ', 'm', 'a', 'c', 'e', 't', 'u', 'd'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis of the merges and vocabularies:\n",
      "\n",
      "Corpus A Vocabulary: {'a b ', ' ', 'a', 'c', 'a ', 'b', 'a b'}\n",
      "Corpus A Merges: [('a', ' '), ('a ', 'b'), ('a b', ' ')]\n",
      "\n",
      "Corpus B Vocabulary: {'o', ',', 's', 'g', 'th', 'h', 'f', 'e ', 'the ', ' ', 'm', 'a', 'c', 'e', 't', 'u', 'd'}\n",
      "Corpus B Merges: [('e', ' '), ('t', 'h'), ('th', 'e ')]\n"
     ]
    }
   ],
   "source": [
    "# Corpus A\n",
    "corpus_a = 'a b c a b c a a b c a a'\n",
    "bpe_a = BPE()\n",
    "print(\"Training BPE on Corpus A...\\n\")\n",
    "vocab_a = bpe_a.train(corpus_a, num_merges=3)\n",
    "print(\"Final Vocabulary for Corpus A:\", vocab_a)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "#  Corpus B\n",
    "corpus_b = 'the dog ate the food, the cat ate the mouse'\n",
    "bpe_b = BPE()\n",
    "print(\"Training BPE on Corpus B...\\n\")\n",
    "vocab_b = bpe_b.train(corpus_b, num_merges=3)\n",
    "print(\"Final Vocabulary for Corpus B:\", vocab_b)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Analysis \n",
    "print(\"Analysis of the merges and vocabularies:\\n\")\n",
    "print(\"Corpus A Vocabulary:\", vocab_a)\n",
    "print(\"Corpus A Merges:\", bpe_a.merges)\n",
    "print()\n",
    "print(\"Corpus B Vocabulary:\", vocab_b)\n",
    "print(\"Corpus B Merges:\", bpe_b.merges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "expected-output-3"
   },
   "source": [
    "#### Expected Output\n",
    "\n",
    "The output should show the final vocabulary for both models, clearly labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "questions-3"
   },
   "source": [
    "## Reflective Questions\n",
    "\n",
    "**1. Compare the vocabularies learned from Corpus A and Corpus B. Why are they different?**  \n",
    "Corpus A produced abstract subwords (short merged sequences of a,b,c) while Corpus B produced meaningful subwords like th, the which are closer to the real words.   \n",
    "\n",
    "**2. How does the structure and content of the training data directly affect the subwords that BPE learns?**  \n",
    "BPE is data-driven. It means that if the training data has repetive short pattern, then there will be abstract tokens. Whereas if the training data is real natural language then the tokens learned will be meaningful subwords and are closer to the real words.  \n",
    "\n",
    "**3. If you were building a BPE model for a large-scale language model, why would it be crucial to use a diverse and representative training corpus?**  \n",
    "If the training corpus is diverse and representative across various disciplines, model will learn balanced vocabulary of subwords that will be generalize across sub-words. On the other hand, if the training corpus is not diverse then model will overfit to frequent patterns in that discipline only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a22248",
   "metadata": {
    "id": "part-4"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 4: Applying BPE to a New Language\n",
    "\n",
    "### Scenario 4: Training BPE on an Urdu corpus\n",
    "\n",
    "This final part extends the BPE implementation to a different language, Urdu, to observe how the algorithm adapts to a new script and linguistic structure.  \n",
    "\n",
    "#### Problem Description\n",
    "\n",
    "You are given a corpus of Urdu text in the file `datasets/urdu_corpus.txt`. Your task is to train your BPE model on this text, analyze the resulting subwords, and tokenize a new sentence.  \n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "1. Print the final vocabulary, including the base characters and the learned merges (use num_merges = 10).\n",
    "2. Tokenize the given test sentence and print the resultant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "code-4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE on Urdu corpus...\n",
      "\n",
      "Initial Tokens: ['ی', 'و', 'ٹ', 'ی', 'و', 'ب', 'ن', 'ے', '8', '0', 'ل', 'ا', 'ک', 'ھ', 'س', 'ے', 'ز', 'ا', 'ئ', 'د', 'خ', 'ل', 'ا', 'ف', 'ض', 'ا', 'ب', 'ط', 'ہ', 'و', 'ی', 'ڈ', 'ی', 'و', 'ز', 'ض', 'ا', 'ئ', 'ع', 'ک', 'ر', 'د', 'ی', 'ں', '\\n', 'ی', 'و', 'ر', 'ی', 'ن']...\n",
      "Initial Vocab: {'ف', 'R', 'r', 'N', 'l', '2', 'D', '9', 'ُ', 'e', 'د', 'V', 'ذ', '‘', 'P', 'ج', '5', 'a', 'ژ', 'ٔ', 'u', 'ح', '!', '6', 'ط', 'ڑ', '’', 'b', 'خ', 'H', 'ا', 'ِ', 'w', 't', ')', '\\xa0', '\\n', ',', 'ٓ', '0', 's', 'َ', 'ن', 'غ', 'ؤ', '3', 'd', 'ر', 'I', '1', 'h', 'چ', 'ء', 'T', 'ص', \"'\", 'ز', 'o', 'ک', '-', 'ت', 'ظ', 'ٰ', 'ب', '…', 'ں', 'و', 'L', ':', 'ئ', 'g', 'y', 'آ', '8', 'ے', 'م', 'س', '،', '؛', 'j', 'Z', '\"', 'ھ', 'k', 'ْ', 'ل', 'ع', 'ہ', 'p', 'M', 'ض', 'B', 'O', 'گ', 'پ', 'ق', '(', 'K', 'm', '7', 'ش', 'ّ', 'ی', 'C', 'ث', 'S', 'A', 'ك', 'ٹ', '۔', '؟', 'c', '.', 'n', 'ي', 'F', '4', 'ڈ', 'i'}\n",
      "\n",
      "\n",
      "Final Vocabulary: {'ف', 'R', 'r', 'N', 'l', '2', 'D', '9', 'ُ', 'e', 'ار', 'د', 'یں', 'V', 'ذ', '‘', 'P', 'ج', '5', 'a', 'ژ', 'ٔ', 'u', 'ح', '!', '6', 'ط', 'ڑ', 'کا', '’', 'b', 'خ', 'H', 'ا', 'ِ', 'w', 't', ')', '\\xa0', '\\n', ',', 'ٓ', '0', 's', 'َ', 'ن', 'غ', 'ؤ', '3', 'd', 'ر', 'I', '1', 'h', 'چ', 'ء', 'T', 'ص', \"'\", 'ز', 'o', 'یا', 'ک', '-', 'ال', 'ت', 'ظ', 'ٰ', 'ب', '…', 'ں', 'و', 'L', ':', 'ئ', 'g', 'y', 'آ', '8', 'ے', 'م', 'س', '،', '؛', 'j', 'Z', '\"', 'ھ', 'k', 'نے', 'کر', 'ْ', 'ل', 'ائ', 'ع', 'ہ', 'p', 'M', 'ض', 'B', 'O', 'گ', 'پ', 'ق', '(', 'K', 'm', '7', 'ش', 'ّ', 'ی', 'ان', 'C', 'ث', 'S', 'A', 'ك', 'ٹ', '۔', '؟', 'c', '.', 'n', 'ي', 'F', 'کی', '4', 'ڈ', 'i'}\n",
      "\n",
      "Tokenized Sentence:\n",
      "['\\n', 'ٹ', 'چ', 'پ', 'ی', 'ڈ', 'ی', 'و', 'ا', 'ی', 'س', 'ب', 'ھ', 'ی', 'کی', 'پ', 'ی', 'ڈ', 'ہ', 'ے', '،', 'ی', 'ع', 'ن', 'یا', 'س', 'ے', 'ی', 'و', 'ا', 'ی', 'س', 'ب', 'ی', 'کی', 'ب', 'ل', 'ک', 'ے', 'ذ', 'ر', 'ی', 'ع', 'ے', 'ک', 'م', 'پ', 'ی', 'و', 'ٹ', 'ر', 'س', 'ے', 'م', 'ن', 'س', 'ل', 'ک', 'ک', 'یا', 'ج', 'ا', 'س', 'ک', 'ت', 'ا', 'ہ', 'ے', '۔', '\\n']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "        self.merges = []\n",
    "\n",
    "    def train(self, text_corpus, num_merges):\n",
    "        # Character-level tokens excluding spaces\n",
    "        tokens = [ch for ch in text_corpus if ch != ' ']\n",
    "        self.vocab = set(tokens)\n",
    "        print(f\"Initial Tokens: {tokens[:50]}...\")  # showing first 50\n",
    "        print(f\"Initial Vocab: {self.vocab}\\n\")\n",
    "        for merge_num in range(1, num_merges+1):\n",
    "            pairs = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
    "            pair_counts = Counter(pairs)\n",
    "            if not pair_counts:\n",
    "                break\n",
    "\n",
    "            # Most frequent pair\n",
    "            most_frequent_pair = pair_counts.most_common(1)[0][0]\n",
    "            self.merges.append(most_frequent_pair)\n",
    "            merged_token = \"\".join(most_frequent_pair)\n",
    "\n",
    "            # Merging tokens\n",
    "            i = 0\n",
    "            new_tokens = []\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens)-1 and (tokens[i], tokens[i+1]) == most_frequent_pair:\n",
    "                    new_tokens.append(merged_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = new_tokens\n",
    "\n",
    "            # Updating vocab\n",
    "            self.vocab.add(merged_token)\n",
    "        return self.vocab\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = [ch for ch in text if ch != ' ']\n",
    "        for pair in self.merges:\n",
    "            merged_token = \"\".join(pair)\n",
    "            i = 0\n",
    "            new_tokens = []\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens)-1 and (tokens[i], tokens[i+1]) == pair:\n",
    "                    new_tokens.append(merged_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "# on urdu_corpus.txt\n",
    "# 1. Loading Urdu corpus\n",
    "with open(\"datasets/urdu_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    urdu_corpus = f.read()\n",
    "bpe_urdu = BPE()\n",
    "\n",
    "# 2. Training with 10 merges\n",
    "print(\"Training BPE on Urdu corpus...\\n\")\n",
    "vocab = bpe_urdu.train(urdu_corpus, num_merges=10)\n",
    "print(\"\\nFinal Vocabulary:\", vocab)\n",
    "\n",
    "# 3. Tokenizing new test sentence\n",
    "test_sentence = \"\"\"\n",
    "ٹچ پیڈ یوایس بھی کی پیڈ ہے، یعنی اسے یو ایس بی کیبل کے ذریعے کمپیوٹر سے منسلک کیا جاسکتا ہے۔\n",
    "\"\"\"\n",
    "tokens = bpe_urdu.tokenize(test_sentence)\n",
    "print(\"\\nTokenized Sentence:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "questions-4"
   },
   "source": [
    "## Reflective Question\n",
    "\n",
    "BPE is called a language-agnostic algorithm; an algorithm does not rely on any pre-existing linguistic knowledge of a language.\n",
    "\n",
    "Comment on the features of the algorithm that make it adaptive to different languages.\n",
    "\n",
    "**Answer:**  \n",
    "BPE does not “know” about words, grammar, or morphology. Instead, it relies on statistics over character sequences. This makes it agnostic to English, Urdu, Chinese, or any other language.  \n",
    "Following are the features of the algorithm that makes it adaptive to different languages:  \n",
    "- BPE always starts with characters and since there are characters in every language, it has a universal starting point.\n",
    "- BPE merges adjacent pairs based purely on frequency and not on grammar. There are no linguistic dependent hard core rules and everything is data driven. Hence, it naturally captures most frequent words/sub words across different languages.\n",
    "- BPE is very robust to new or OOV words. If a word isn't in the vocab then BPE falls back to smaller units (characters).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_pa1)",
   "language": "python",
   "name": "nlp_pa1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
